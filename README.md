For now the task is to create a scraper from liquidpedia. The goal is to scrape data about 2v2 Starcraft results, based on players, race, maps, and tournaments. We will be starting with the uterman 2v2 circut. https://liquipedia.net/starcraft2/UThermal_2v2_Circuit/Main_Event 

see @docs/initial_promt.md for basic idea

**Data Storage**: The scraper exports data to JSON files, which are then inserted into **Supabase PostgreSQL database** via direct PostgreSQL connection, enabling efficient bulk data insertion and advanced analytics.

# Enhanced SC2 Stats Scraper

This enhanced scraper system provides robust web scraping for Liquipedia tournament data with advanced features like caching, error handling, and batch processing. **Data is exported to JSON and then inserted into Supabase via direct PostgreSQL connection for efficient data ingestion.**

## Features

### ðŸš€ **Performance & Scalability**
- **Async support** with `aiohttp` for concurrent requests
- **Controlled concurrency** to avoid overwhelming servers
- **Batch processing** for multiple tournaments
- **Priority-based task scheduling**

### ðŸ’¾ **Caching & Efficiency**
- **TTL-based caching** to avoid re-scraping
- **Smart cache invalidation** based on configurable timeouts
- **Cache statistics** and monitoring
- **Persistent file-based caching**

### ðŸ›¡ï¸ **Reliability & Error Handling**
- **Automatic retries** with exponential backoff
- **Rate limiting** to respect server limits
- **Comprehensive error handling** and logging
- **Graceful degradation** on failures

### ðŸ—„ï¸ **Database Integration**
- **JSON export** for data persistence and transfer
- **Direct PostgreSQL connection** for efficient bulk data insertion
- **Structured data persistence** in PostgreSQL tables
- **Data validation** and integrity checks

### ðŸ”§ **Configuration & Monitoring**
- **Environment-based configuration**
- **Detailed logging** with configurable levels
- **Performance metrics** and statistics
- **Session reporting** and analysis

## Architecture

```
EnhancedScraper
â”œâ”€â”€ Task Management
â”‚   â”œâ”€â”€ Priority-based scheduling
â”‚   â”œâ”€â”€ Retry logic with backoff
â”‚   â””â”€â”€ Concurrent processing control
â”œâ”€â”€ Data Processing
â”‚   â”œâ”€â”€ MediaWiki API client
â”‚   â”œâ”€â”€ HTML parsing with BeautifulSoup
â”‚   â””â”€â”€ Structured data extraction
â”œâ”€â”€ Caching Layer
â”‚   â”œâ”€â”€ In-memory TTL cache
â”‚   â”œâ”€â”€ Persistent file storage
â”‚   â””â”€â”€ Cache validation and cleanup
â”œâ”€â”€ Database Layer
â”‚   â”œâ”€â”€ Direct PostgreSQL connection
â”‚   â”œâ”€â”€ Data transformation & validation
â”‚   â”œâ”€â”€ Schema management
â”‚   â””â”€â”€ Transaction handling
â””â”€â”€ Output Management
    â”œâ”€â”€ Database persistence
    â”œâ”€â”€ Statistics and reporting
    â””â”€â”€ Error tracking and analysis
```

## Data Flow

```
Liquipedia API â†’ Enhanced Scraper â†’ JSON Export â†’ Direct PostgreSQL â†’ Supabase Database
     â†“                    â†“              â†“              â†“              â†“
  Raw Data          Cached Data    Structured    Bulk Insertion    Data Storage
  Extraction        Management      Data         via psycopg2     for Web Apps
```

## Configuration Options

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `LIQUIPEDIA_USER_AGENT` | `sc2stats/1.0` | User agent for API requests |
| `RATE_LIMIT_DELAY` | `1.0` | Delay between requests (seconds) |
| `MAX_RETRIES` | `5` | Maximum retry attempts |
| `CACHE_TTL` | `3600` | Cache time-to-live (seconds) |
| `SUPABASE_URL` | - | Supabase project URL |
| `SUPABASE_SERVICE_KEY` | - | Supabase service role key (for direct PostgreSQL access) |
| `DATABASE_URL` | - | Direct PostgreSQL connection string |
| `CACHE_DIR` | `cache` | Cache storage directory |

## Database Schema

The scraper will populate the following Supabase tables:

- **`tournaments`**: Tournament metadata, dates, locations
- **`players`**: Player information, race preferences, statistics
- **`matches`**: Match results, brackets, scores
- **`games`**: Individual map results, map types, game details
- **`teams`**: 2v2 team compositions and partnerships

> ðŸ“‹ **Detailed Schema**: See `docs/database_schema.py` for complete SQL DDL statements and table definitions.

## Architecture Decision

**Why Direct PostgreSQL Connection?**

The scraper uses a direct PostgreSQL connection instead of MCP tools or Supabase client for the following reasons:

- **Performance**: Direct connection provides optimal performance for bulk data insertion
- **Reliability**: Eliminates dependency on external MCP tool availability
- **Control**: Full control over SQL queries and transaction management
- **Simplicity**: Single Python environment handles both scraping and database operations

**Frontend Integration**

The React frontend will use the Supabase client for:
- Data fetching (one-time, no real-time subscriptions needed)
- User authentication and RLS policies
- Client-side data manipulation (filtering, sorting, graph redrawing)

This architecture provides the best of both worlds: efficient bulk insertion from the scraper and rich data access capabilities in the frontend.

## File Structure

```
tools/scraper/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ scraper_config.py        # Configuration management
â”œâ”€â”€ liquipedia_client.py     # MediaWiki API client with caching
â”œâ”€â”€ data_models.py           # Data structures and models
â”œâ”€â”€ data_parser.py           # Unified parser for wikitext and LPDB data
â”œâ”€â”€ scraper.py               # Main scraper script (exports to JSON)
â”œâ”€â”€ database_schema.py       # Database schema definitions (in docs/)
â”œâ”€â”€ cache/                   # Cached API responses
â””â”€â”€ README.md                # This file
```
