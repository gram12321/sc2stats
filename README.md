# SC2 2v2 Stats Scraper

A robust web scraper for Liquipedia tournament data that extracts StarCraft 2 2v2 tournament information and stores it in a Supabase PostgreSQL database.

## Overview

This scraper system provides comprehensive data extraction for SC2 2v2 tournaments from Liquipedia, with features like caching, error handling, and batch processing. **Data is exported to JSON and then inserted into Supabase via the Supabase Python client for efficient data ingestion.**

## Features

### ðŸš€ **Performance & Scalability**
- **Async support** with `aiohttp` for concurrent requests
- **Controlled concurrency** to avoid overwhelming servers
- **Batch processing** for multiple tournaments
- **Priority-based task scheduling**

### ðŸ’¾ **Caching & Efficiency**
- **TTL-based caching** to avoid re-scraping
- **Smart cache invalidation** based on configurable timeouts
- **Cache statistics** and monitoring
- **Persistent file-based caching**

### ðŸ›¡ï¸ **Reliability & Error Handling**
- **Automatic retries** with exponential backoff
- **Rate limiting** to respect server limits
- **Comprehensive error handling** and logging
- **Graceful degradation** on failures

### ðŸ—„ï¸ **Database Integration**
- **JSON export** for data persistence and transfer
- **Supabase Python client** for efficient data insertion
- **Structured data persistence** in PostgreSQL tables
- **Data validation** and integrity checks

### ðŸ”§ **Configuration & Monitoring**
- **Environment-based configuration**
- **Detailed logging** with configurable levels
- **Performance metrics** and statistics
- **Session reporting** and analysis

## Architecture

```
EnhancedScraper
â”œâ”€â”€ Task Management
â”‚   â”œâ”€â”€ Priority-based scheduling
â”‚   â”œâ”€â”€ Retry logic with backoff
â”‚   â””â”€â”€ Concurrent processing control
â”œâ”€â”€ Data Processing
â”‚   â”œâ”€â”€ MediaWiki API client
â”‚   â”œâ”€â”€ HTML parsing with BeautifulSoup
â”‚   â””â”€â”€ Structured data extraction
â”œâ”€â”€ Caching Layer
â”‚   â”œâ”€â”€ In-memory TTL cache
â”‚   â”œâ”€â”€ Persistent file storage
â”‚   â””â”€â”€ Cache validation and cleanup
â”œâ”€â”€ Database Layer
â”‚   â”œâ”€â”€ Supabase Python client
â”‚   â”œâ”€â”€ Data transformation & validation
â”‚   â”œâ”€â”€ Schema management
â”‚   â””â”€â”€ Transaction handling
â””â”€â”€ Output Management
    â”œâ”€â”€ Database persistence
    â”œâ”€â”€ Statistics and reporting
    â””â”€â”€ Error tracking and analysis
```

## Data Flow

```
Liquipedia API â†’ Enhanced Scraper â†’ JSON Export â†’ Supabase Client â†’ Supabase Database
     â†“                    â†“              â†“              â†“              â†“
  Raw Data          Cached Data    Structured    Bulk Insertion    Data Storage
  Extraction        Management      Data         via Supabase      for Web Apps
```

## Configuration Options

| Environment Variable | Default | Description |
|---------------------|---------|-------------|
| `LIQUIPEDIA_USER_AGENT` | `sc2stats/1.0` | User agent for API requests |
| `RATE_LIMIT_DELAY` | `1.0` | Delay between requests (seconds) |
| `MAX_RETRIES` | `5` | Maximum retry attempts |
| `CACHE_TTL` | `3600` | Cache time-to-live (seconds) |
| `SUPABASE_URL` | - | Supabase project URL |
| `SUPABASE_ANON_KEY` | - | Supabase anonymous key |
| `CACHE_DIR` | `cache` | Cache storage directory |

## Database Schema

The scraper populates the following Supabase tables:

- **`tournaments`**: Tournament metadata, dates, locations
- **`players`**: Player information, race preferences, statistics
- **`matches`**: Match results, brackets, scores
- **`games`**: Individual map results, map types, game details
- **`teams`**: 2v2 team compositions and partnerships

> ðŸ“‹ **Detailed Schema**: See `docs/database_schema.py` for complete SQL DDL statements and table definitions.

## Architecture Decision

**Why Supabase Python Client?**

The scraper uses the Supabase Python client instead of direct PostgreSQL connections for the following reasons:

- **Simplicity**: Leverages Supabase's built-in authentication and security
- **Reliability**: Handles connection pooling and retries automatically
- **Security**: Uses Supabase's RLS policies and authentication
- **Maintenance**: Easier to maintain and update

**Frontend Integration**

The React frontend will use the Supabase client for:
- Data fetching and real-time subscriptions
- User authentication and RLS policies
- Client-side data manipulation (filtering, sorting, graph redrawing)

## File Structure

```
tools/scraper/
â”œâ”€â”€ __init__.py              # Package initialization
â”œâ”€â”€ scraper_config.py        # Configuration management
â”œâ”€â”€ liquipedia_client.py     # MediaWiki API client with caching
â”œâ”€â”€ data_models.py           # Data structures and models
â”œâ”€â”€ data_parser.py           # Unified parser for wikitext and LPDB data
â”œâ”€â”€ scraper.py               # Main scraper script (exports to JSON)
â”œâ”€â”€ database_inserter.py     # Supabase database integration
â”œâ”€â”€ database_schema.py       # Database schema definitions (in docs/)
â”œâ”€â”€ cache/                   # Cached API responses
â””â”€â”€ README.md                # This file
```